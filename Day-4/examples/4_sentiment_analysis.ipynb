{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "from torchtext.data import Field, LabelField, TabularDataset, Iterator\n",
    "from torchtext.vocab import Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(object):\n",
    "\n",
    "    def __init__(self, root_dir='data/sentiment/', batch_size=64, use_vector=True):\n",
    "        self.TEXT = Field(sequential=True, use_vocab=True,\n",
    "                          tokenize='spacy', lower=True, batch_first=True)\n",
    "        self.LABEL = LabelField()\n",
    "        vectors = Vectors(name='data/sentiment/mr_vocab.txt', cache='./')\n",
    "\n",
    "        dataset_path = os.path.join(root_dir, '{}.tsv')\n",
    "        self.dataset = {}\n",
    "        self.dataloader = {}\n",
    "        for target in ['train', 'dev', 'test']:\n",
    "            self.dataset[target] = TabularDataset(\n",
    "                path=dataset_path.format(target),\n",
    "                format='tsv',\n",
    "                fields=[('text', self.TEXT), ('label', self.LABEL)]\n",
    "            )\n",
    "            if use_vector:\n",
    "                self.TEXT.build_vocab(self.dataset[target], max_size=25000, vectors=vectors)\n",
    "            else:\n",
    "                self.TEXT.build_vocab(self.dataset[target], max_size=25000)\n",
    "\n",
    "            self.LABEL.build_vocab(self.dataset[target])\n",
    "            self.dataloader[target] = Iterator(self.dataset[target],\n",
    "                                               batch_size=batch_size,\n",
    "                                               device=None,\n",
    "                                               repeat=False,\n",
    "                                               sort_key=lambda x: len(x.text),\n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes,\n",
    "                 output_dim, use_dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters,\n",
    "                      kernel_size=(fs, embedding_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5 if use_dropout else 0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0)\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    # round predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    # convert into float for division\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# device = 'cpu'\n",
    "dataset = MyDataset(batch_size=64, use_vector=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(len(dataset.TEXT.vocab), 300, 100, [3, 4, 5], 1, True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
       "        ...,\n",
       "        [ 0.2679,  0.1430,  0.4684,  ...,  0.0252, -0.1573,  0.4174],\n",
       "        [-0.1195,  0.3609,  0.0184,  ..., -0.6871, -0.6613, -0.2103],\n",
       "        [ 0.2707, -0.0874, -0.3683,  ...,  0.0368, -0.2172, -0.3406]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(dataset.TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(size_average=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        batch.text = batch.text.permute(1, 0)\n",
    "        pred = model(batch.text.to(device)).squeeze(1)\n",
    "        loss = criterion(pred, batch.label.double().to(device))\n",
    "        acc = binary_accuracy(pred, batch.label.double().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        batch.text = batch.text.permute(1, 0)\n",
    "        pred = model(batch.text.to(device)).squeeze(1)\n",
    "        loss = criterion(pred, batch.label.double().to(device))\n",
    "        acc = binary_accuracy(pred, batch.label.double().to(device))\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.699, Train Acc: 48.30%, Val. Loss: 0.694, Val. Acc: 48.44%, Test Loss: 0.696, Test Acc: 46.11%\n",
      "Epoch: 02, Train Loss: 0.699, Train Acc: 49.25%, Val. Loss: 0.694, Val. Acc: 48.34%, Test Loss: 0.695, Test Acc: 48.13%\n",
      "Epoch: 03, Train Loss: 0.697, Train Acc: 49.83%, Val. Loss: 0.693, Val. Acc: 50.18%, Test Loss: 0.694, Test Acc: 49.96%\n",
      "Epoch: 04, Train Loss: 0.697, Train Acc: 49.95%, Val. Loss: 0.692, Val. Acc: 51.34%, Test Loss: 0.693, Test Acc: 51.38%\n",
      "Epoch: 05, Train Loss: 0.695, Train Acc: 50.73%, Val. Loss: 0.691, Val. Acc: 52.39%, Test Loss: 0.692, Test Acc: 52.21%\n",
      "Epoch: 06, Train Loss: 0.694, Train Acc: 50.59%, Val. Loss: 0.691, Val. Acc: 53.40%, Test Loss: 0.691, Test Acc: 53.43%\n",
      "Epoch: 07, Train Loss: 0.693, Train Acc: 51.71%, Val. Loss: 0.690, Val. Acc: 54.51%, Test Loss: 0.690, Test Acc: 55.01%\n",
      "Epoch: 08, Train Loss: 0.693, Train Acc: 52.66%, Val. Loss: 0.689, Val. Acc: 56.07%, Test Loss: 0.690, Test Acc: 55.90%\n",
      "Epoch: 09, Train Loss: 0.691, Train Acc: 52.85%, Val. Loss: 0.689, Val. Acc: 57.13%, Test Loss: 0.689, Test Acc: 57.29%\n",
      "Epoch: 10, Train Loss: 0.690, Train Acc: 52.53%, Val. Loss: 0.688, Val. Acc: 58.01%, Test Loss: 0.688, Test Acc: 58.10%\n",
      "Epoch: 11, Train Loss: 0.690, Train Acc: 52.80%, Val. Loss: 0.687, Val. Acc: 58.68%, Test Loss: 0.687, Test Acc: 58.48%\n",
      "Epoch: 12, Train Loss: 0.689, Train Acc: 53.92%, Val. Loss: 0.686, Val. Acc: 59.56%, Test Loss: 0.686, Test Acc: 59.02%\n",
      "Epoch: 13, Train Loss: 0.687, Train Acc: 54.22%, Val. Loss: 0.686, Val. Acc: 60.08%, Test Loss: 0.685, Test Acc: 60.09%\n",
      "Epoch: 14, Train Loss: 0.688, Train Acc: 54.40%, Val. Loss: 0.685, Val. Acc: 60.44%, Test Loss: 0.685, Test Acc: 60.09%\n",
      "Epoch: 15, Train Loss: 0.688, Train Acc: 54.59%, Val. Loss: 0.684, Val. Acc: 61.73%, Test Loss: 0.684, Test Acc: 62.17%\n",
      "Epoch: 16, Train Loss: 0.686, Train Acc: 54.77%, Val. Loss: 0.684, Val. Acc: 61.35%, Test Loss: 0.683, Test Acc: 60.09%\n",
      "Epoch: 17, Train Loss: 0.686, Train Acc: 54.98%, Val. Loss: 0.683, Val. Acc: 62.32%, Test Loss: 0.682, Test Acc: 62.43%\n",
      "Epoch: 18, Train Loss: 0.683, Train Acc: 55.76%, Val. Loss: 0.682, Val. Acc: 63.35%, Test Loss: 0.681, Test Acc: 63.53%\n",
      "Epoch: 19, Train Loss: 0.684, Train Acc: 56.58%, Val. Loss: 0.682, Val. Acc: 62.55%, Test Loss: 0.681, Test Acc: 62.98%\n",
      "Epoch: 20, Train Loss: 0.681, Train Acc: 57.31%, Val. Loss: 0.681, Val. Acc: 63.61%, Test Loss: 0.680, Test Acc: 64.26%\n",
      "Epoch: 21, Train Loss: 0.682, Train Acc: 57.07%, Val. Loss: 0.680, Val. Acc: 64.17%, Test Loss: 0.679, Test Acc: 64.41%\n",
      "Epoch: 22, Train Loss: 0.681, Train Acc: 57.94%, Val. Loss: 0.680, Val. Acc: 64.16%, Test Loss: 0.678, Test Acc: 65.30%\n",
      "Epoch: 23, Train Loss: 0.680, Train Acc: 58.06%, Val. Loss: 0.679, Val. Acc: 63.88%, Test Loss: 0.677, Test Acc: 64.41%\n",
      "Epoch: 24, Train Loss: 0.678, Train Acc: 58.75%, Val. Loss: 0.678, Val. Acc: 65.32%, Test Loss: 0.676, Test Acc: 65.55%\n",
      "Epoch: 25, Train Loss: 0.678, Train Acc: 58.37%, Val. Loss: 0.678, Val. Acc: 65.22%, Test Loss: 0.676, Test Acc: 65.23%\n",
      "Epoch: 26, Train Loss: 0.678, Train Acc: 58.66%, Val. Loss: 0.677, Val. Acc: 65.63%, Test Loss: 0.675, Test Acc: 65.37%\n",
      "Epoch: 27, Train Loss: 0.676, Train Acc: 59.81%, Val. Loss: 0.676, Val. Acc: 64.90%, Test Loss: 0.674, Test Acc: 65.50%\n",
      "Epoch: 28, Train Loss: 0.676, Train Acc: 59.07%, Val. Loss: 0.675, Val. Acc: 64.43%, Test Loss: 0.673, Test Acc: 65.95%\n",
      "Epoch: 29, Train Loss: 0.676, Train Acc: 59.28%, Val. Loss: 0.675, Val. Acc: 65.31%, Test Loss: 0.672, Test Acc: 65.77%\n",
      "Epoch: 30, Train Loss: 0.674, Train Acc: 60.32%, Val. Loss: 0.674, Val. Acc: 65.54%, Test Loss: 0.671, Test Acc: 65.60%\n",
      "Epoch: 31, Train Loss: 0.672, Train Acc: 60.46%, Val. Loss: 0.673, Val. Acc: 65.54%, Test Loss: 0.670, Test Acc: 65.61%\n",
      "Epoch: 32, Train Loss: 0.670, Train Acc: 61.49%, Val. Loss: 0.672, Val. Acc: 65.72%, Test Loss: 0.669, Test Acc: 65.91%\n",
      "Epoch: 33, Train Loss: 0.671, Train Acc: 61.15%, Val. Loss: 0.671, Val. Acc: 66.06%, Test Loss: 0.668, Test Acc: 66.14%\n",
      "Epoch: 34, Train Loss: 0.669, Train Acc: 61.85%, Val. Loss: 0.670, Val. Acc: 65.64%, Test Loss: 0.668, Test Acc: 65.67%\n",
      "Epoch: 35, Train Loss: 0.668, Train Acc: 61.35%, Val. Loss: 0.670, Val. Acc: 65.73%, Test Loss: 0.667, Test Acc: 66.10%\n",
      "Epoch: 36, Train Loss: 0.669, Train Acc: 61.23%, Val. Loss: 0.669, Val. Acc: 66.52%, Test Loss: 0.665, Test Acc: 66.68%\n",
      "Epoch: 37, Train Loss: 0.669, Train Acc: 61.28%, Val. Loss: 0.668, Val. Acc: 66.75%, Test Loss: 0.664, Test Acc: 67.09%\n",
      "Epoch: 38, Train Loss: 0.666, Train Acc: 62.09%, Val. Loss: 0.668, Val. Acc: 66.69%, Test Loss: 0.664, Test Acc: 67.15%\n",
      "Epoch: 39, Train Loss: 0.667, Train Acc: 62.00%, Val. Loss: 0.666, Val. Acc: 66.37%, Test Loss: 0.663, Test Acc: 66.93%\n",
      "Epoch: 40, Train Loss: 0.665, Train Acc: 62.66%, Val. Loss: 0.666, Val. Acc: 66.37%, Test Loss: 0.662, Test Acc: 67.29%\n",
      "Epoch: 41, Train Loss: 0.665, Train Acc: 62.53%, Val. Loss: 0.665, Val. Acc: 66.70%, Test Loss: 0.661, Test Acc: 67.71%\n",
      "Epoch: 42, Train Loss: 0.662, Train Acc: 63.35%, Val. Loss: 0.664, Val. Acc: 66.60%, Test Loss: 0.660, Test Acc: 67.96%\n",
      "Epoch: 43, Train Loss: 0.664, Train Acc: 62.41%, Val. Loss: 0.663, Val. Acc: 66.83%, Test Loss: 0.659, Test Acc: 67.53%\n",
      "Epoch: 44, Train Loss: 0.661, Train Acc: 63.28%, Val. Loss: 0.662, Val. Acc: 67.36%, Test Loss: 0.657, Test Acc: 68.25%\n",
      "Epoch: 45, Train Loss: 0.659, Train Acc: 62.94%, Val. Loss: 0.661, Val. Acc: 67.11%, Test Loss: 0.657, Test Acc: 68.12%\n",
      "Epoch: 46, Train Loss: 0.658, Train Acc: 64.00%, Val. Loss: 0.660, Val. Acc: 67.08%, Test Loss: 0.656, Test Acc: 68.62%\n",
      "Epoch: 47, Train Loss: 0.657, Train Acc: 63.78%, Val. Loss: 0.659, Val. Acc: 67.10%, Test Loss: 0.654, Test Acc: 68.45%\n",
      "Epoch: 48, Train Loss: 0.657, Train Acc: 64.04%, Val. Loss: 0.659, Val. Acc: 67.20%, Test Loss: 0.653, Test Acc: 68.31%\n",
      "Epoch: 49, Train Loss: 0.655, Train Acc: 64.73%, Val. Loss: 0.657, Val. Acc: 67.21%, Test Loss: 0.652, Test Acc: 68.82%\n",
      "Epoch: 50, Train Loss: 0.653, Train Acc: 64.65%, Val. Loss: 0.657, Val. Acc: 67.48%, Test Loss: 0.651, Test Acc: 69.21%\n",
      "Epoch: 51, Train Loss: 0.652, Train Acc: 64.56%, Val. Loss: 0.655, Val. Acc: 68.08%, Test Loss: 0.650, Test Acc: 69.05%\n",
      "Epoch: 52, Train Loss: 0.652, Train Acc: 65.40%, Val. Loss: 0.655, Val. Acc: 67.38%, Test Loss: 0.648, Test Acc: 69.89%\n",
      "Epoch: 53, Train Loss: 0.651, Train Acc: 64.74%, Val. Loss: 0.653, Val. Acc: 67.80%, Test Loss: 0.648, Test Acc: 69.37%\n",
      "Epoch: 54, Train Loss: 0.649, Train Acc: 64.83%, Val. Loss: 0.652, Val. Acc: 67.75%, Test Loss: 0.646, Test Acc: 69.69%\n",
      "Epoch: 55, Train Loss: 0.648, Train Acc: 65.05%, Val. Loss: 0.651, Val. Acc: 67.89%, Test Loss: 0.645, Test Acc: 69.76%\n",
      "Epoch: 56, Train Loss: 0.647, Train Acc: 65.46%, Val. Loss: 0.650, Val. Acc: 67.56%, Test Loss: 0.644, Test Acc: 69.39%\n",
      "Epoch: 57, Train Loss: 0.646, Train Acc: 65.96%, Val. Loss: 0.650, Val. Acc: 67.80%, Test Loss: 0.643, Test Acc: 69.68%\n",
      "Epoch: 58, Train Loss: 0.643, Train Acc: 66.02%, Val. Loss: 0.648, Val. Acc: 68.03%, Test Loss: 0.641, Test Acc: 70.49%\n",
      "Epoch: 59, Train Loss: 0.643, Train Acc: 66.22%, Val. Loss: 0.647, Val. Acc: 68.03%, Test Loss: 0.639, Test Acc: 70.63%\n",
      "Epoch: 60, Train Loss: 0.644, Train Acc: 65.59%, Val. Loss: 0.646, Val. Acc: 67.89%, Test Loss: 0.639, Test Acc: 70.30%\n",
      "Epoch: 61, Train Loss: 0.641, Train Acc: 66.36%, Val. Loss: 0.644, Val. Acc: 68.17%, Test Loss: 0.638, Test Acc: 70.11%\n",
      "Epoch: 62, Train Loss: 0.640, Train Acc: 65.82%, Val. Loss: 0.643, Val. Acc: 68.50%, Test Loss: 0.636, Test Acc: 70.66%\n",
      "Epoch: 63, Train Loss: 0.638, Train Acc: 66.29%, Val. Loss: 0.642, Val. Acc: 68.31%, Test Loss: 0.634, Test Acc: 70.94%\n",
      "Epoch: 64, Train Loss: 0.637, Train Acc: 66.49%, Val. Loss: 0.641, Val. Acc: 68.16%, Test Loss: 0.633, Test Acc: 71.06%\n",
      "Epoch: 65, Train Loss: 0.633, Train Acc: 67.90%, Val. Loss: 0.640, Val. Acc: 68.73%, Test Loss: 0.631, Test Acc: 71.27%\n",
      "Epoch: 66, Train Loss: 0.634, Train Acc: 67.46%, Val. Loss: 0.639, Val. Acc: 68.35%, Test Loss: 0.630, Test Acc: 71.25%\n",
      "Epoch: 67, Train Loss: 0.632, Train Acc: 66.91%, Val. Loss: 0.637, Val. Acc: 68.49%, Test Loss: 0.629, Test Acc: 70.55%\n",
      "Epoch: 68, Train Loss: 0.632, Train Acc: 67.11%, Val. Loss: 0.637, Val. Acc: 68.57%, Test Loss: 0.628, Test Acc: 71.26%\n",
      "Epoch: 69, Train Loss: 0.628, Train Acc: 67.40%, Val. Loss: 0.635, Val. Acc: 69.23%, Test Loss: 0.626, Test Acc: 71.42%\n",
      "Epoch: 70, Train Loss: 0.629, Train Acc: 67.29%, Val. Loss: 0.634, Val. Acc: 69.23%, Test Loss: 0.625, Test Acc: 71.42%\n",
      "Epoch: 71, Train Loss: 0.631, Train Acc: 66.59%, Val. Loss: 0.632, Val. Acc: 69.18%, Test Loss: 0.623, Test Acc: 71.55%\n",
      "Epoch: 72, Train Loss: 0.625, Train Acc: 67.91%, Val. Loss: 0.631, Val. Acc: 68.86%, Test Loss: 0.622, Test Acc: 71.05%\n",
      "Epoch: 73, Train Loss: 0.624, Train Acc: 67.65%, Val. Loss: 0.630, Val. Acc: 69.31%, Test Loss: 0.621, Test Acc: 71.47%\n",
      "Epoch: 74, Train Loss: 0.623, Train Acc: 68.40%, Val. Loss: 0.629, Val. Acc: 68.76%, Test Loss: 0.619, Test Acc: 71.88%\n",
      "Epoch: 75, Train Loss: 0.624, Train Acc: 67.56%, Val. Loss: 0.627, Val. Acc: 68.96%, Test Loss: 0.617, Test Acc: 71.64%\n",
      "Epoch: 76, Train Loss: 0.622, Train Acc: 68.28%, Val. Loss: 0.626, Val. Acc: 69.24%, Test Loss: 0.616, Test Acc: 71.92%\n",
      "Epoch: 77, Train Loss: 0.617, Train Acc: 68.87%, Val. Loss: 0.625, Val. Acc: 69.18%, Test Loss: 0.615, Test Acc: 71.51%\n",
      "Epoch: 78, Train Loss: 0.618, Train Acc: 68.58%, Val. Loss: 0.624, Val. Acc: 68.99%, Test Loss: 0.613, Test Acc: 71.38%\n",
      "Epoch: 79, Train Loss: 0.617, Train Acc: 68.75%, Val. Loss: 0.622, Val. Acc: 69.42%, Test Loss: 0.612, Test Acc: 71.65%\n",
      "Epoch: 80, Train Loss: 0.618, Train Acc: 68.52%, Val. Loss: 0.621, Val. Acc: 69.36%, Test Loss: 0.610, Test Acc: 71.61%\n",
      "Epoch: 81, Train Loss: 0.614, Train Acc: 68.88%, Val. Loss: 0.620, Val. Acc: 69.18%, Test Loss: 0.609, Test Acc: 71.49%\n",
      "Epoch: 82, Train Loss: 0.613, Train Acc: 69.12%, Val. Loss: 0.618, Val. Acc: 69.51%, Test Loss: 0.607, Test Acc: 71.79%\n",
      "Epoch: 83, Train Loss: 0.609, Train Acc: 69.39%, Val. Loss: 0.618, Val. Acc: 69.50%, Test Loss: 0.606, Test Acc: 72.28%\n",
      "Epoch: 84, Train Loss: 0.611, Train Acc: 69.46%, Val. Loss: 0.617, Val. Acc: 69.63%, Test Loss: 0.604, Test Acc: 72.47%\n",
      "Epoch: 85, Train Loss: 0.610, Train Acc: 68.91%, Val. Loss: 0.615, Val. Acc: 69.27%, Test Loss: 0.603, Test Acc: 71.93%\n",
      "Epoch: 86, Train Loss: 0.605, Train Acc: 69.96%, Val. Loss: 0.614, Val. Acc: 69.91%, Test Loss: 0.601, Test Acc: 72.29%\n",
      "Epoch: 87, Train Loss: 0.606, Train Acc: 68.80%, Val. Loss: 0.612, Val. Acc: 69.31%, Test Loss: 0.599, Test Acc: 72.11%\n",
      "Epoch: 88, Train Loss: 0.604, Train Acc: 69.39%, Val. Loss: 0.611, Val. Acc: 69.91%, Test Loss: 0.598, Test Acc: 72.29%\n",
      "Epoch: 89, Train Loss: 0.602, Train Acc: 69.43%, Val. Loss: 0.610, Val. Acc: 69.60%, Test Loss: 0.596, Test Acc: 72.51%\n",
      "Epoch: 90, Train Loss: 0.601, Train Acc: 69.67%, Val. Loss: 0.608, Val. Acc: 69.50%, Test Loss: 0.595, Test Acc: 72.56%\n",
      "Epoch: 91, Train Loss: 0.602, Train Acc: 69.86%, Val. Loss: 0.608, Val. Acc: 69.35%, Test Loss: 0.594, Test Acc: 71.97%\n",
      "Epoch: 92, Train Loss: 0.599, Train Acc: 69.82%, Val. Loss: 0.605, Val. Acc: 69.79%, Test Loss: 0.591, Test Acc: 72.79%\n",
      "Epoch: 93, Train Loss: 0.598, Train Acc: 69.83%, Val. Loss: 0.604, Val. Acc: 69.55%, Test Loss: 0.591, Test Acc: 72.26%\n",
      "Epoch: 94, Train Loss: 0.597, Train Acc: 69.73%, Val. Loss: 0.603, Val. Acc: 69.41%, Test Loss: 0.589, Test Acc: 72.56%\n",
      "Epoch: 95, Train Loss: 0.596, Train Acc: 70.09%, Val. Loss: 0.603, Val. Acc: 69.73%, Test Loss: 0.587, Test Acc: 72.83%\n",
      "Epoch: 96, Train Loss: 0.593, Train Acc: 70.11%, Val. Loss: 0.601, Val. Acc: 70.19%, Test Loss: 0.586, Test Acc: 72.65%\n",
      "Epoch: 97, Train Loss: 0.590, Train Acc: 70.70%, Val. Loss: 0.599, Val. Acc: 69.92%, Test Loss: 0.585, Test Acc: 72.74%\n",
      "Epoch: 98, Train Loss: 0.590, Train Acc: 70.53%, Val. Loss: 0.598, Val. Acc: 69.64%, Test Loss: 0.584, Test Acc: 72.67%\n",
      "Epoch: 99, Train Loss: 0.588, Train Acc: 70.30%, Val. Loss: 0.596, Val. Acc: 69.97%, Test Loss: 0.582, Test Acc: 72.85%\n",
      "Epoch: 100, Train Loss: 0.587, Train Acc: 70.72%, Val. Loss: 0.595, Val. Acc: 70.38%, Test Loss: 0.579, Test Acc: 72.96%\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(100):\n",
    "    train_loss, train_acc = train(model, dataset.dataloader['train'],\n",
    "                                  optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, dataset.dataloader['dev'],\n",
    "                                     criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, dataset.dataloader['test'],\n",
    "                                   criterion, device)\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc * 100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc * 100:.2f}%, Test Loss: {test_loss:.3f}, Test Acc: {test_acc * 100:.2f}%')\n",
    "\n",
    "    if best_acc <= valid_acc:\n",
    "        best_acc = valid_acc\n",
    "        acc_result = test_acc\n",
    "        pth = model.state_dict()\n",
    "        filename = \"checkpoints/{}_{}_bs{}_filter{}_acc{:.03f}.pth\".format(\n",
    "                \"CNN\", \"SGD\", \"64\", 100, test_acc\n",
    "            )\n",
    "torch.save(model, filename)\n",
    "\n",
    "print(\"training done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('checkpoints/CNN_SGD_bs64_filter100_acc0.730.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6402595639228821\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"Holland’s film manages to really get under one’s skin on the whole, \n",
    "remaining a compelling and engaging watch throughout in spite of its rambling feel. \n",
    "This is in large part thanks to the awesome first-time writer Andrea Chalupa’s \n",
    "clear-eyed resolve to find present-day relevancy in Jones’s heroic \n",
    "commitment to publishing the facts and agitating those in power, \n",
    "when today’s world has almost become numb to fake news. It's a \n",
    "fun and exciting movie to watch. Overall we give the movie a 7/10 score\n",
    "to enjoy with your friends and family. 👍\"\"\"\n",
    "\n",
    "dataset = MyDataset(batch_size=1, use_vector=True)\n",
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "indexed = [dataset.TEXT.vocab.stoi[t] for t in tokenized]\n",
    "tensor = torch.LongTensor(indexed).to(device)\n",
    "tensor = tensor.unsqueeze(1)\n",
    "prediction = F.sigmoid(model(tensor))\n",
    "print(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5298455357551575\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"While “Mr. Jones” isn’t close to being in the same league as \n",
    "great journalism films such as “All The President’s Men” and “Spotlight,” \n",
    "it takes a noble page from their book in not delivering a conclusive ending. \n",
    "Instead, it leaves things at a place where the work to uncover “only one version of the truth” \n",
    "(as often cited by characters in the film) seems to be just beginning. \n",
    "This might not be the ultimate movie to honor the idealistic legacy of Gareth Jones, but it’s a dignified one all the same, with an uncompromising moral compass.\"\"\"\n",
    "\n",
    "dataset = MyDataset(batch_size=1, use_vector=True)\n",
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "indexed = [dataset.TEXT.vocab.stoi[t] for t in tokenized]\n",
    "tensor = torch.LongTensor(indexed).to(device)\n",
    "tensor = tensor.unsqueeze(1)\n",
    "prediction = F.sigmoid(model(tensor))\n",
    "print(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4953272044658661\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"They say it takes a village to raise a child, but in “Safety,” \n",
    "it takes an entire university campus to do so. Overly sentimental traps line \n",
    "the plot of the film, streaming on Disney+. But it scores points for giving \n",
    "its lead characters complicated situations, emotional depth and political dimension.\"\"\"\n",
    "\n",
    "dataset = MyDataset(batch_size=1, use_vector=True)\n",
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "indexed = [dataset.TEXT.vocab.stoi[t] for t in tokenized]\n",
    "tensor = torch.LongTensor(indexed).to(device)\n",
    "tensor = tensor.unsqueeze(1)\n",
    "prediction = F.sigmoid(model(tensor))\n",
    "print(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4291599690914154\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"Not only does Ruth succeed in fulfilling the promise, \n",
    "but also the child is subsequently cured of his cancer. Baseball player \n",
    "Ted Williams believed it to be the worst movie he had ever seen 👎 and \n",
    "The Washington Times stated that it \"stands as possibly the worst movie ever made\" 😒.\n",
    "The film has been called one of the worst sports films ever by Newsday and The A.V. Club\"\"\"\n",
    "\n",
    "dataset = MyDataset(batch_size=1, use_vector=True)\n",
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "indexed = [dataset.TEXT.vocab.stoi[t] for t in tokenized]\n",
    "tensor = torch.LongTensor(indexed).to(device)\n",
    "tensor = tensor.unsqueeze(1)\n",
    "prediction = F.sigmoid(model(tensor))\n",
    "print(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
