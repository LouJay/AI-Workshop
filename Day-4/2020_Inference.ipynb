{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as Functional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "flatten = itertools.chain.from_iterable\n",
    "\n",
    "# Some helper functions\n",
    "\n",
    "def plot_loss(loss_as_list):\n",
    "    \"\"\"\n",
    "    Plot the loss curve from a list of loss terms.\n",
    "    \"\"\"\n",
    "    plt.plot(loss_as_list, 'k')\n",
    "    _ = plt.title(\"Loss Curve\")\n",
    "    _ = plt.xlabel(\"Epochs\")\n",
    "    _ = plt.ylabel(\"Loss\")\n",
    "    \n",
    "def get_classification_results(model, loader):\n",
    "    \"\"\"\n",
    "    Print the accuracy of a trained model.\n",
    "    Loss: Cross Entropy\n",
    "    \"\"\"\n",
    "    correct, total = 0, 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for xs, ts in test_loader:\n",
    "        zs = model(xs) # do forward pass\n",
    "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        correct += pred.eq(ts.view_as(pred)).sum().item() # count equal values\n",
    "        total += int(ts.shape[0]) # get total values\n",
    "\n",
    "        predictions.append(pred)\n",
    "        true_labels.append(ts)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    conf_matrix = confusion_matrix(list(flatten(true_labels)), list(flatten(predictions)))\n",
    "    cl_report = classification_report(list(flatten(true_labels)), list(flatten(predictions)), digits=4)\n",
    "\n",
    "    print(cl_report)\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "N_train = 64\n",
    "N_test = 256\n",
    "\n",
    "# We will use torch.utils.data.DataLoader to wrap our dataset.\n",
    "# This provides easier batching, GPU support, etc.\n",
    "# Calling torchvision.datasets.MNIST() will download and format the MNIST\n",
    "# dataset with the transforms we specify. Here, in the transforms we first convert\n",
    "# the image to PyTorch tensor, and then normalize the image based on a given mean\n",
    "# and standard deviation. Normalizing the image does: image = (image - mean) / std.\n",
    "# We shuffle the data as well by defining shuffle=True.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = enumerate(test_loader)\n",
    "batch_idx, (one_batch_of_test_subset_x, one_batch_of_test_subset_y) = next(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "plt.imshow(one_batch_of_test_subset_x[i][0], cmap='gray', interpolation='none')\n",
    "_ = plt.title(\"Ground Truth: {}\".format(one_batch_of_test_subset_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CNN_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_A, self).__init__()\n",
    "        # We can define the arguments of each layer in the __init__ method.\n",
    "        # __init__ method will be called everytime we create an object of this class.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This is the forward pass function.\n",
    "        # See how we can save the activation outputs of each layer into a variable.\n",
    "        # In this case, we are saving the output of each layer\n",
    "        # to the same variable and replacing the value every time\n",
    "        # before sending to a new layers.\n",
    "        x = self.conv1(x)\n",
    "        x = Functional.max_pool2d(x, 2)\n",
    "        x = Functional.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = Functional.max_pool2d(x, 2)\n",
    "        x = Functional.relu(x)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = Functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = Functional.relu(x) ## UPDATE ::\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('cnn_a_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_A(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000, 11.9198, 11.9203,  0.0000,  0.0000,  0.0000, 33.1333,\n",
      "          0.0000,  7.4134]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Extract the output logit\n",
    "output_logits = model(one_batch_of_test_subset_x[i].reshape(1,1,28,28)) # remember you need a 4D tensor of NCHW format.\n",
    "print(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0774e-15, 4.0774e-15, 6.1249e-10, 6.1280e-10, 4.0774e-15, 4.0774e-15,\n",
      "         4.0774e-15, 1.0000e+00, 4.0774e-15, 6.7602e-12]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Extract Probability vector\n",
    "smax = torch.nn.Softmax(dim=1)\n",
    "softmax_out = smax(torch.nn.functional.relu(output_logits))\n",
    "print(softmax_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([[1.]], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([[7]]))\n"
     ]
    }
   ],
   "source": [
    "# Find the max probability location\n",
    "max_predictions = softmax_out.max(1, keepdim=True)\n",
    "print(max_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Find the label with max probability\n",
    "max_predicted_label = max_predictions.indices.item()\n",
    "print(max_predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
